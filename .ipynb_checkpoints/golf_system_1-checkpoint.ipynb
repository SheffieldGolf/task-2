{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tr = pd.read_csv('data/train/tweets.queries.tsv', sep='\\t', header=0, index_col=0)\n",
    "tweets_tr.sort_index(inplace=True)\n",
    "\n",
    "tweets_te = pd.read_csv('data/dev/tweets.queries.tsv', sep='\\t', header=0, index_col=0)\n",
    "tweets_te.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump needs to immediately divest from his bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A number of fraudulent text messages informing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fact check: The U.S. Army is NOT contacting an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The US drone attack on #Soleimani caught on ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1. To the dim witted reporters like @dmedin11:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>@HillaryClinton Wikileaks exposed the fact tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>WIKILEAKS BOMBSHELL: Hillary Clinton Smeared P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Clown Mask To Be Banned In USA; Wearing One Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>The snowfall this year is supposed to be \"reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>there's gonna be 10-20 times more snow than th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>803 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_content\n",
       "1     Trump needs to immediately divest from his bus...\n",
       "2     A number of fraudulent text messages informing...\n",
       "3     Fact check: The U.S. Army is NOT contacting an...\n",
       "4     The US drone attack on #Soleimani caught on ca...\n",
       "5     1. To the dim witted reporters like @dmedin11:...\n",
       "...                                                 ...\n",
       "997   @HillaryClinton Wikileaks exposed the fact tha...\n",
       "998   WIKILEAKS BOMBSHELL: Hillary Clinton Smeared P...\n",
       "999   Clown Mask To Be Banned In USA; Wearing One Co...\n",
       "1001  The snowfall this year is supposed to be \"reco...\n",
       "1002  there's gonna be 10-20 times more snow than th...\n",
       "\n",
       "[803 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vclaims = pd.read_csv('data/verified_claims.docs.tsv', sep='\\t', header=0, index_col=0)\n",
    "vclaims.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vclaim</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122 detainees released from confinement at Gua...</td>\n",
       "      <td>Did 122 Prisoners Released from Guantanamo by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A \"Trump and Obama by the Numbers\" meme recoun...</td>\n",
       "      <td>Does This Meme Accurately Show ‘Trump and Obam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A \"large-scale killing\" of white farmers is ta...</td>\n",
       "      <td>Is a ‘Large-Scale Killing’ of White Farmers Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A \"law to separate families\" was enacted prior...</td>\n",
       "      <td>Was the ‘Law to Separate Families’ Passed in 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A \"newly uncovered\" photograph reveals Alexand...</td>\n",
       "      <td>Does an Image Show Ocasio-Cortez Fake-Crying a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>WikiLeaks was caught by Newsweek fabricating e...</td>\n",
       "      <td>Newsweek Proves That WikiLeaks Is Leaking Phon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>Wikileaks released a trove of \"deep state file...</td>\n",
       "      <td>Did WikiLeaks Release a Trove of ‘Deep State F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>Wombats are herding animals and inviting them ...</td>\n",
       "      <td>Are Wombats Inviting Animals Into Their Burrow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>YETI told the NRA the brand no longer wishes t...</td>\n",
       "      <td>Did YETI Brand Coolers Cut Ties with the NRA?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>“Presidential alerts” are capable of accessing...</td>\n",
       "      <td>Do Presidential Alerts Give the Government Tot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>784 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                vclaim  \\\n",
       "0    122 detainees released from confinement at Gua...   \n",
       "1    A \"Trump and Obama by the Numbers\" meme recoun...   \n",
       "2    A \"large-scale killing\" of white farmers is ta...   \n",
       "3    A \"law to separate families\" was enacted prior...   \n",
       "4    A \"newly uncovered\" photograph reveals Alexand...   \n",
       "..                                                 ...   \n",
       "779  WikiLeaks was caught by Newsweek fabricating e...   \n",
       "780  Wikileaks released a trove of \"deep state file...   \n",
       "781  Wombats are herding animals and inviting them ...   \n",
       "782  YETI told the NRA the brand no longer wishes t...   \n",
       "783  “Presidential alerts” are capable of accessing...   \n",
       "\n",
       "                                                 title  \n",
       "0    Did 122 Prisoners Released from Guantanamo by ...  \n",
       "1    Does This Meme Accurately Show ‘Trump and Obam...  \n",
       "2    Is a ‘Large-Scale Killing’ of White Farmers Un...  \n",
       "3    Was the ‘Law to Separate Families’ Passed in 1...  \n",
       "4    Does an Image Show Ocasio-Cortez Fake-Crying a...  \n",
       "..                                                 ...  \n",
       "779  Newsweek Proves That WikiLeaks Is Leaking Phon...  \n",
       "780  Did WikiLeaks Release a Trove of ‘Deep State F...  \n",
       "781  Are Wombats Inviting Animals Into Their Burrow...  \n",
       "782      Did YETI Brand Coolers Cut Ties with the NRA?  \n",
       "783  Do Presidential Alerts Give the Government Tot...  \n",
       "\n",
       "[784 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vclaims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_tr = pd.read_csv('data/train/tweet-vclaim-pairs.qrels', sep='\\t', \n",
    "                       header=None, names=['tweet_id', '0', 'vclaim_id', 'relevance'])\n",
    "\n",
    "qrels_te = pd.read_csv('data/dev/tweet-vclaim-pairs.qrels', sep='\\t', \n",
    "                       header=None, names=['tweet_id', '0', 'vclaim_id', 'relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>0</th>\n",
       "      <th>vclaim_id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>395</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>998</td>\n",
       "      <td>0</td>\n",
       "      <td>462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>1002</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>804 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id  0  vclaim_id  relevance\n",
       "0           1  0        395          1\n",
       "1           2  0        669          1\n",
       "2           3  0        669          1\n",
       "3           4  0        144          1\n",
       "4           5  0         84          1\n",
       "..        ... ..        ...        ...\n",
       "799       997  0        197          1\n",
       "800       998  0        462          1\n",
       "801       999  0        260          1\n",
       "802      1001  0        207          1\n",
       "803      1002  0        207          1\n",
       "\n",
       "[804 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Raw Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "url_pattern = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "username_pattern = r\"@[^\\s]+\"\n",
    "hashtag_pattern = r\"\\B#\\w\\w+\"\n",
    "token_pattern = r\"\\b[A-Za-z][A-Za-z]+\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of preprocessing tweet\n",
    "def preprocess_tweet(tweet, \n",
    "                     url_pattern=url_pattern, username_pattern=username_pattern, \n",
    "                     hashtag_pattern=hashtag_pattern, token_pattern=token_pattern, \n",
    "                     remove_url=True, remove_username=True, remove_hashtag=True,\n",
    "                     stopwords=stopwords, with_stopwordsrm=True, with_stemming=True):\n",
    "    # remove content after '—'\n",
    "    tweet = tweet.split('—')[0]\n",
    "    \n",
    "    # remove url\n",
    "    if remove_url == True:\n",
    "        tweet = re.sub(url_pattern, \"\", tweet)\n",
    "        \n",
    "    # remove @username \n",
    "    if remove_username == True:\n",
    "        tweet = re.sub(username_pattern, \"\", tweet)\n",
    "        \n",
    "    # remove #hashtag\n",
    "    if remove_hashtag == True:\n",
    "        tweet = re.sub(hashtag_pattern, \"\", tweet)\n",
    "    \n",
    "    # lower case \n",
    "    tweet_lower = tweet.lower()\n",
    "    \n",
    "    # tokenization \n",
    "    words = re.findall(token_pattern, tweet_lower)\n",
    "    \n",
    "    # stopwords removal\n",
    "    if with_stopwordsrm == True:\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        \n",
    "    # stemming \n",
    "    if with_stemming == True:\n",
    "        ps = PorterStemmer() \n",
    "        words = [ps.stem(word) for word in words]\n",
    "        \n",
    "    tweet_processed = \" \".join(words)\n",
    "    \n",
    "    return tweet_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict tweets_tr: tweet_id -> tweet_content\n",
    "tweets_tr_prep = {}\n",
    "for tweet_id in tweets_tr.index:\n",
    "    tweets_tr_prep[tweet_id] = preprocess_tweet(tweets_tr.loc[tweet_id, 'tweet_content'])\n",
    "\n",
    "# dict tweets_te: tweet_id -> tweet_content\n",
    "tweets_te_prep = {}\n",
    "for tweet_id in tweets_te.index:\n",
    "    tweets_te_prep[tweet_id] = preprocess_tweet(tweets_te.loc[tweet_id, 'tweet_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biden grand wizard kkk play lie use vote creator kkk oppos civil right black yup democrat parti'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_tr_prep[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of preprocessing vclaim\n",
    "def preprocess_text(text, token_pattern=token_pattern, stopwords=stopwords, \n",
    "                      with_stopwordsrm=True, with_stemming=True):\n",
    "    # lower case \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # tokenization \n",
    "    words = re.findall(token_pattern, text_lower)\n",
    "    \n",
    "    # stopwords removal\n",
    "    if with_stopwordsrm == True:\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        \n",
    "    # stemming \n",
    "    if with_stemming == True:\n",
    "        ps = PorterStemmer() \n",
    "        words = [ps.stem(word) for word in words]\n",
    "        \n",
    "    text_processed = \" \".join(words)\n",
    "    \n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict vclaim_prep: vlciam_id -> [vlciam_content, vclaim_title]\n",
    "vclaims_prep = {}\n",
    "for vclaim_id in vclaims.index:\n",
    "    vclaims_prep[vclaim_id] = []\n",
    "    vclaims_prep[vclaim_id].append(preprocess_text(vclaims.loc[vclaim_id, 'vclaim']))\n",
    "    vclaims_prep[vclaim_id].append(preprocess_text(vclaims.loc[vclaim_id, 'title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Cosine Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of computing consine similarity \n",
    "def compute_cs(tweets, vclaims):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vclaims_tfidf = vectorizer.fit_transform(vclaims)\n",
    "    \n",
    "    cosine_sims = {}\n",
    "    for (tweet_id, tweet_content) in tweets.items():\n",
    "        tweet_tfidf = vectorizer.transform([tweet_content])\n",
    "        cs = cosine_similarity(tweet_tfidf, vclaims_tfidf).flatten()\n",
    "        cosine_sims[tweet_id] = cs\n",
    "    \n",
    "    return cosine_sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vclaims_contents = [vclaim[0] for vclaim in vclaims_prep.values()]\n",
    "vclaims_titles = [vclaim[1] for vclaim in vclaims_prep.values()]\n",
    "\n",
    "cs_tvc_tr = compute_cs(tweets_tr_prep, vclaims_contents)\n",
    "cs_tvt_tr = compute_cs(tweets_tr_prep, vclaims_titles)\n",
    "\n",
    "cs_tvc_te = compute_cs(tweets_te_prep, vclaims_contents)\n",
    "cs_tvt_te = compute_cs(tweets_te_prep, vclaims_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cs_tvc_tr = pd.DataFrame.from_dict(cs_tvc_tr, orient='index')\n",
    "df_cs_tvt_tr = pd.DataFrame.from_dict(cs_tvt_tr, orient='index')\n",
    "\n",
    "df_cs_tvc_te = pd.DataFrame.from_dict(cs_tvc_te, orient='index')\n",
    "df_cs_tvt_te = pd.DataFrame.from_dict(cs_tvt_te, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing BM25 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of computing BM25 score\n",
    "def compute_bm25(tokenized_tweets, tokenized_vclaims):\n",
    "    bm25kapi = BM25Okapi(tokenized_vclaims)\n",
    "    \n",
    "    bm25s = {}\n",
    "    for (tweet_id, tweet_content) in tokenized_tweets.items():\n",
    "        bm25s[tweet_id] = bm25kapi.get_scores(tweet_content)\n",
    "        \n",
    "    return bm25s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_vclaims_contents = [vclaim[0].split(\" \") for vclaim in vclaims_prep.values()]\n",
    "tokenized_vclaims_titles = [vclaim[1].split(\" \") for vclaim in vclaims_prep.values()]\n",
    "\n",
    "tokenized_tweets_tr = {}\n",
    "for (tweet_id, tweet_content) in tweets_tr_prep.items():\n",
    "    tokenized_tweets_tr[tweet_id] = tweet_content.split(\" \")\n",
    "    \n",
    "tokenized_tweets_te = {}\n",
    "for (tweet_id, tweet_content) in tweets_te_prep.items():\n",
    "    tokenized_tweets_te[tweet_id] = tweet_content.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_tvc_tr = compute_bm25(tokenized_tweets_tr, tokenized_vclaims_contents)\n",
    "bm25_tvt_tr = compute_bm25(tokenized_tweets_tr, tokenized_vclaims_titles)\n",
    "\n",
    "bm25_tvc_te = compute_bm25(tokenized_tweets_te, tokenized_vclaims_contents)\n",
    "bm25_tvt_te = compute_bm25(tokenized_tweets_te, tokenized_vclaims_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm25_tvc_tr = pd.DataFrame.from_dict(bm25_tvc_tr, orient='index')\n",
    "df_bm25_tvt_tr = pd.DataFrame.from_dict(bm25_tvt_tr, orient='index')\n",
    "\n",
    "df_bm25_tvc_te = pd.DataFrame.from_dict(bm25_tvc_te, orient='index')\n",
    "df_bm25_tvt_te = pd.DataFrame.from_dict(bm25_tvt_te, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct dataset with 4 features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr_tids = []\n",
    "for tweet_id in tweets_tr_prep.keys():\n",
    "    data_tr_tids.extend((np.ones(vclaims.shape[0]) * tweet_id).tolist())\n",
    "    \n",
    "data_tr_vids = []\n",
    "for i in range(tweets_tr.shape[0]):\n",
    "    data_tr_vids.extend(list(vclaims_prep.keys()))\n",
    "    \n",
    "data_tr_labels = []\n",
    "for tweet_id in tweets_tr_prep.keys():\n",
    "    labels = np.zeros(vclaims.shape[0])\n",
    "    if tweet_id in qrels_tr['tweet_id'].values:\n",
    "        for index in qrels_tr[qrels_tr['tweet_id'] == tweet_id]['vclaim_id'].values:\n",
    "            labels[index] = 1\n",
    "    data_tr_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = pd.DataFrame(columns=['tid', 'vid', 'cs_tvc', 'cs_tvt', 'bm25_tvc', 'bm25_tvt', 'label'])\n",
    "data_tr['tid'] = data_tr_tids\n",
    "data_tr['vid'] = data_tr_vids\n",
    "data_tr['cs_tvc'] = df_cs_tvc_tr.values.flatten()\n",
    "data_tr['cs_tvt'] = df_cs_tvt_tr.values.flatten()\n",
    "data_tr['bm25_tvc'] = df_bm25_tvc_tr.values.flatten()\n",
    "data_tr['bm25_tvt'] = df_bm25_tvt_tr.values.flatten()\n",
    "data_tr['label'] = data_tr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_te_tids = []\n",
    "for tweet_id in tweets_te_prep.keys():\n",
    "    data_te_tids.extend((np.ones(vclaims.shape[0]) * tweet_id).tolist())\n",
    "    \n",
    "data_te_vids = []\n",
    "for i in range(tweets_te.shape[0]):\n",
    "    data_te_vids.extend(list(vclaims_prep.keys()))\n",
    "    \n",
    "data_te_labels = []\n",
    "for tweet_id in tweets_te_prep.keys():\n",
    "    labels = np.zeros(vclaims.shape[0])\n",
    "    if tweet_id in qrels_te['tweet_id'].values:\n",
    "        for index in qrels_te[qrels_te['tweet_id'] == tweet_id]['vclaim_id'].values:\n",
    "            labels[index] = 1\n",
    "    data_te_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_te = pd.DataFrame(columns=['tid', 'vid', 'cs_tvc', 'cs_tvt', 'bm25_tvc', 'bm25_tvt', 'label'])\n",
    "data_te['tid'] = data_te_tids\n",
    "data_te['vid'] = data_te_vids\n",
    "data_te['cs_tvc'] = df_cs_tvc_te.values.flatten()\n",
    "data_te['cs_tvt'] = df_cs_tvt_te.values.flatten()\n",
    "data_te['bm25_tvc'] = df_bm25_tvc_te.values.flatten()\n",
    "data_te['bm25_tvt'] = df_bm25_tvt_te.values.flatten()\n",
    "data_te['label'] = data_te_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = data_tr[['cs_tvc', 'cs_tvt', 'bm25_tvc', 'bm25_tvt']].values, data_tr['label'].values\n",
    "X_te, y_te = data_te[['cs_tvc', 'cs_tvt', 'bm25_tvc', 'bm25_tvt']].values, data_te['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {}\n",
    "for label in set(y_te):\n",
    "    class_weight[label] = np.sum(y_te == label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/administrator/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(C=0.1, random_state=0, class_weight=class_weight, max_iter=20000).fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = np.sum(X_te * clf.coef_, axis=1) + clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_score = pd.DataFrame(columns=['tid','vid', 'score'])\n",
    "result_score['tid'] = data_te['tid']\n",
    "result_score['vid'] = data_te['vid']\n",
    "result_score['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['tweet_id', 'Q0', 'vclaim_id', 'rank', 'score', 'tag'])\n",
    "\n",
    "tid = list(result_score.groupby(by='tid').groups.keys())\n",
    "for i in tid:\n",
    "    idx = result_score[result_score['tid'] == i]['score'].idxmax()\n",
    "    inf = result_score.iloc[idx]\n",
    "    result = result.append({'tweet_id':int(inf[0]), 'Q0':'Q0', 'vclaim_id':int(inf[1]),\n",
    "                            'rank':1, 'score':inf[2], 'tag': 'COVID-19'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>Q0</th>\n",
       "      <th>vclaim_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Q0</td>\n",
       "      <td>456</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.057749</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>Q0</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.410604</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>Q0</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.788070</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>Q0</td>\n",
       "      <td>223</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.542566</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Q0</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.944743</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>971</td>\n",
       "      <td>Q0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.179441</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>983</td>\n",
       "      <td>Q0</td>\n",
       "      <td>439</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.215035</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>984</td>\n",
       "      <td>Q0</td>\n",
       "      <td>439</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.018836</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>989</td>\n",
       "      <td>Q0</td>\n",
       "      <td>668</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.131253</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1000</td>\n",
       "      <td>Q0</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530065</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id  Q0 vclaim_id rank     score       tag\n",
       "0          0  Q0       456    1 -1.057749  COVID-19\n",
       "1         12  Q0       640    1 -0.410604  COVID-19\n",
       "2         23  Q0       109    1 -0.788070  COVID-19\n",
       "3         30  Q0       223    1 -0.542566  COVID-19\n",
       "4         33  Q0       171    1 -0.944743  COVID-19\n",
       "..       ...  ..       ...  ...       ...       ...\n",
       "195      971  Q0        38    1 -0.179441  COVID-19\n",
       "196      983  Q0       439    1 -0.215035  COVID-19\n",
       "197      984  Q0       439    1 -0.018836  COVID-19\n",
       "198      989  Q0       668    1 -1.131253  COVID-19\n",
       "199     1000  Q0       181    1  0.530065  COVID-19\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('golf_system_result_0.csv', header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 evaluate.py -s golf_system_result_0.csv -g data/dev/tweet-vclaim-pairs.qrels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
