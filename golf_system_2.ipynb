{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tr = pd.read_csv('data/train/tweets.queries.tsv', sep='\\t', header=0, index_col=0)\n",
    "tweets_tr.sort_index(inplace=True)\n",
    "\n",
    "tweets_te = pd.read_csv('data/dev/tweets.queries.tsv', sep='\\t', header=0, index_col=0)\n",
    "tweets_te.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump needs to immediately divest from his bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A number of fraudulent text messages informing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fact check: The U.S. Army is NOT contacting an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The US drone attack on #Soleimani caught on ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1. To the dim witted reporters like @dmedin11:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Giants stars dare NFL to fine them for cleats ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Fury over NFL's crackdown on player's 9/11 tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>During a recent interview with Australian jour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>#BreakingNews: We’re launching an exciting new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Phyllis Schlafly said “We’ll have a woman Pres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tweet_content\n",
       "1    Trump needs to immediately divest from his bus...\n",
       "2    A number of fraudulent text messages informing...\n",
       "3    Fact check: The U.S. Army is NOT contacting an...\n",
       "4    The US drone attack on #Soleimani caught on ca...\n",
       "5    1. To the dim witted reporters like @dmedin11:...\n",
       "..                                                 ...\n",
       "994  Giants stars dare NFL to fine them for cleats ...\n",
       "995  Fury over NFL's crackdown on player's 9/11 tri...\n",
       "996  During a recent interview with Australian jour...\n",
       "997  #BreakingNews: We’re launching an exciting new...\n",
       "998  Phyllis Schlafly said “We’ll have a woman Pres...\n",
       "\n",
       "[800 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vclaims = pd.read_csv('data/verified_claims.docs.tsv', sep='\\t', header=0, index_col=0)\n",
    "vclaims.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vclaim</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122 detainees released from confinement at Gua...</td>\n",
       "      <td>Did 122 Prisoners Released from Guantanamo by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70 per cent of the persons arrested during pro...</td>\n",
       "      <td>70% of Arrested Charlotte Protesters Are Out-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A \"Trump and Obama by the Numbers\" meme recoun...</td>\n",
       "      <td>Does This Meme Accurately Show ‘Trump and Obam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A \"large-scale killing\" of white farmers is ta...</td>\n",
       "      <td>Is a ‘Large-Scale Killing’ of White Farmers Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A \"law to separate families\" was enacted prior...</td>\n",
       "      <td>Was the ‘Law to Separate Families’ Passed in 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10370</th>\n",
       "      <td>“Slime,” a do-it-yourself gooey craft project ...</td>\n",
       "      <td>Does the “Slime” Craze Bring Serious Health Ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10371</th>\n",
       "      <td>“Sun tea” (tea brewed by being left to steep i...</td>\n",
       "      <td>Bacteria in Sun Tea Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10372</th>\n",
       "      <td>“The Real Deal,” words of wisdom about gas, ge...</td>\n",
       "      <td>Red Thomas ‘Real Deal’ Letter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10373</th>\n",
       "      <td>“Valentine’s Day” worm.</td>\n",
       "      <td>Valentine’s Day Worm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10374</th>\n",
       "      <td>“WTC Survivor.”</td>\n",
       "      <td>WTC Survivor Virus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10375 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  vclaim  \\\n",
       "0      122 detainees released from confinement at Gua...   \n",
       "1      70 per cent of the persons arrested during pro...   \n",
       "2      A \"Trump and Obama by the Numbers\" meme recoun...   \n",
       "3      A \"large-scale killing\" of white farmers is ta...   \n",
       "4      A \"law to separate families\" was enacted prior...   \n",
       "...                                                  ...   \n",
       "10370  “Slime,” a do-it-yourself gooey craft project ...   \n",
       "10371  “Sun tea” (tea brewed by being left to steep i...   \n",
       "10372  “The Real Deal,” words of wisdom about gas, ge...   \n",
       "10373                            “Valentine’s Day” worm.   \n",
       "10374                                    “WTC Survivor.”   \n",
       "\n",
       "                                                   title  \n",
       "0      Did 122 Prisoners Released from Guantanamo by ...  \n",
       "1      70% of Arrested Charlotte Protesters Are Out-o...  \n",
       "2      Does This Meme Accurately Show ‘Trump and Obam...  \n",
       "3      Is a ‘Large-Scale Killing’ of White Farmers Un...  \n",
       "4      Was the ‘Law to Separate Families’ Passed in 1...  \n",
       "...                                                  ...  \n",
       "10370  Does the “Slime” Craze Bring Serious Health Ri...  \n",
       "10371                           Bacteria in Sun Tea Risk  \n",
       "10372                      Red Thomas ‘Real Deal’ Letter  \n",
       "10373                               Valentine’s Day Worm  \n",
       "10374                                 WTC Survivor Virus  \n",
       "\n",
       "[10375 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vclaims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_tr = pd.read_csv('data/train/tweet-vclaim-pairs.qrels', sep='\\t', \n",
    "                       header=None, names=['tweet_id', '0', 'vclaim_id', 'relevance'])\n",
    "\n",
    "qrels_te = pd.read_csv('data/dev/tweet-vclaim-pairs.qrels', sep='\\t', \n",
    "                       header=None, names=['tweet_id', '0', 'vclaim_id', 'relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>0</th>\n",
       "      <th>vclaim_id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>994</td>\n",
       "      <td>0</td>\n",
       "      <td>652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>995</td>\n",
       "      <td>0</td>\n",
       "      <td>652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>996</td>\n",
       "      <td>0</td>\n",
       "      <td>778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>998</td>\n",
       "      <td>0</td>\n",
       "      <td>539</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>801 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id  0  vclaim_id  relevance\n",
       "0           1  0        394          1\n",
       "1           2  0        670          1\n",
       "2           3  0        670          1\n",
       "3           4  0        141          1\n",
       "4           5  0         83          1\n",
       "..        ... ..        ...        ...\n",
       "796       994  0        652          1\n",
       "797       995  0        652          1\n",
       "798       996  0        778          1\n",
       "799       997  0        579          1\n",
       "800       998  0        539          1\n",
       "\n",
       "[801 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Raw Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "url_pattern = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "username_pattern = r\"@[^\\s]+\"\n",
    "hashtag_pattern = r\"\\B#\\w\\w+\"\n",
    "token_pattern = r\"\\b[A-Za-z][A-Za-z]+\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of preprocessing tweet\n",
    "def preprocess_tweet(tweet, \n",
    "                     url_pattern=url_pattern, username_pattern=username_pattern, \n",
    "                     hashtag_pattern=hashtag_pattern, token_pattern=token_pattern, \n",
    "                     remove_url=True, remove_username=True, remove_hashtag=True,\n",
    "                     stopwords=stopwords, with_stopwordsrm=True, with_stemming=True):\n",
    "    # remove content after '—'\n",
    "    tweet = tweet.split('—')[0]\n",
    "    \n",
    "    # remove url\n",
    "    if remove_url == True:\n",
    "        tweet = re.sub(url_pattern, \"\", tweet)\n",
    "        \n",
    "    # remove @username \n",
    "    if remove_username == True:\n",
    "        tweet = re.sub(username_pattern, \"\", tweet)\n",
    "        \n",
    "    # remove #hashtag\n",
    "    if remove_hashtag == True:\n",
    "        tweet = re.sub(hashtag_pattern, \"\", tweet)\n",
    "    \n",
    "    # lower case \n",
    "    tweet_lower = tweet.lower()\n",
    "    \n",
    "    # tokenization \n",
    "    words = re.findall(token_pattern, tweet_lower)\n",
    "    \n",
    "    # stopwords removal\n",
    "    if with_stopwordsrm == True:\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        \n",
    "    # stemming \n",
    "    if with_stemming == True:\n",
    "        ps = PorterStemmer() \n",
    "        words = [ps.stem(word) for word in words]\n",
    "        \n",
    "    tweet_processed = \" \".join(words)\n",
    "    \n",
    "    return tweet_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict tweets_tr: tweet_id -> tweet_content\n",
    "tweets_tr_prep = {}\n",
    "for tweet_id in tweets_tr.index:\n",
    "    tweets_tr_prep[tweet_id] = preprocess_tweet(tweets_tr.loc[tweet_id, 'tweet_content'])\n",
    "\n",
    "# dict tweets_te: tweet_id -> tweet_content\n",
    "tweets_te_prep = {}\n",
    "for tweet_id in tweets_te.index:\n",
    "    tweets_te_prep[tweet_id] = preprocess_tweet(tweets_te.loc[tweet_id, 'tweet_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of preprocessing vclaim\n",
    "def preprocess_text(text, token_pattern=token_pattern, stopwords=stopwords, \n",
    "                      with_stopwordsrm=True, with_stemming=True):\n",
    "    # lower case \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # tokenization \n",
    "    words = re.findall(token_pattern, text_lower)\n",
    "    \n",
    "    # stopwords removal\n",
    "    if with_stopwordsrm == True:\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        \n",
    "    # stemming \n",
    "    if with_stemming == True:\n",
    "        ps = PorterStemmer() \n",
    "        words = [ps.stem(word) for word in words]\n",
    "        \n",
    "    text_processed = \" \".join(words)\n",
    "    \n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict vclaim_prep: vlciam_id -> [vlciam_content, vclaim_title]\n",
    "vclaims_prep = {}\n",
    "for vclaim_id in vclaims.index:\n",
    "    vclaims_prep[vclaim_id] = []\n",
    "    vclaims_prep[vclaim_id].append(preprocess_text(vclaims.loc[vclaim_id, 'vclaim']))\n",
    "    vclaims_prep[vclaim_id].append(preprocess_text(vclaims.loc[vclaim_id, 'title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Cosine Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of computing consine similarity \n",
    "def compute_cs(tweets, vclaims):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vclaims_tfidf = vectorizer.fit_transform(vclaims)\n",
    "    \n",
    "    cosine_sims = {}\n",
    "    for (tweet_id, tweet_content) in tweets.items():\n",
    "        tweet_tfidf = vectorizer.transform([tweet_content])\n",
    "        cs = cosine_similarity(tweet_tfidf, vclaims_tfidf).flatten()\n",
    "        cosine_sims[tweet_id] = cs\n",
    "    \n",
    "    return cosine_sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing Cosine Similarities for train and test set\n",
    "vclaims_contents = [vclaim[0] for vclaim in vclaims_prep.values()]\n",
    "vclaims_titles = [vclaim[1] for vclaim in vclaims_prep.values()]\n",
    "\n",
    "cs_tvc_tr = compute_cs(tweets_tr_prep, vclaims_contents)\n",
    "cs_tvt_tr = compute_cs(tweets_tr_prep, vclaims_titles)\n",
    "\n",
    "cs_tvc_te = compute_cs(tweets_te_prep, vclaims_contents)\n",
    "cs_tvt_te = compute_cs(tweets_te_prep, vclaims_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cs_tvc_tr = pd.DataFrame.from_dict(cs_tvc_tr, orient='index')\n",
    "df_cs_tvt_tr = pd.DataFrame.from_dict(cs_tvt_tr, orient='index')\n",
    "\n",
    "df_cs_tvc_te = pd.DataFrame.from_dict(cs_tvc_te, orient='index')\n",
    "df_cs_tvt_te = pd.DataFrame.from_dict(cs_tvt_te, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing BM25 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of computing BM25 score\n",
    "def compute_bm25(tokenized_tweets, tokenized_vclaims):\n",
    "    bm25kapi = BM25Okapi(tokenized_vclaims)\n",
    "    \n",
    "    bm25s = {}\n",
    "    for (tweet_id, tweet_content) in tokenized_tweets.items():\n",
    "        bm25s[tweet_id] = bm25kapi.get_scores(tweet_content)\n",
    "        \n",
    "    return bm25s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing pre-processed text\n",
    "tokenized_vclaims_contents = [vclaim[0].split(\" \") for vclaim in vclaims_prep.values()]\n",
    "tokenized_vclaims_titles = [vclaim[1].split(\" \") for vclaim in vclaims_prep.values()]\n",
    "\n",
    "tokenized_tweets_tr = {}\n",
    "for (tweet_id, tweet_content) in tweets_tr_prep.items():\n",
    "    tokenized_tweets_tr[tweet_id] = tweet_content.split(\" \")\n",
    "    \n",
    "tokenized_tweets_te = {}\n",
    "for (tweet_id, tweet_content) in tweets_te_prep.items():\n",
    "    tokenized_tweets_te[tweet_id] = tweet_content.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing BM25 Scores for train and test set\n",
    "bm25_tvc_tr = compute_bm25(tokenized_tweets_tr, tokenized_vclaims_contents)\n",
    "bm25_tvt_tr = compute_bm25(tokenized_tweets_tr, tokenized_vclaims_titles)\n",
    "\n",
    "bm25_tvc_te = compute_bm25(tokenized_tweets_te, tokenized_vclaims_contents)\n",
    "bm25_tvt_te = compute_bm25(tokenized_tweets_te, tokenized_vclaims_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm25_tvc_tr = pd.DataFrame.from_dict(bm25_tvc_tr, orient='index')\n",
    "df_bm25_tvt_tr = pd.DataFrame.from_dict(bm25_tvt_tr, orient='index')\n",
    "\n",
    "df_bm25_tvc_te = pd.DataFrame.from_dict(bm25_tvc_te, orient='index')\n",
    "df_bm25_tvt_te = pd.DataFrame.from_dict(bm25_tvt_te, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing dataset with 4 features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr_tids = []\n",
    "for tweet_id in tweets_tr_prep.keys():\n",
    "    data_tr_tids.extend((np.ones(vclaims.shape[0]) * tweet_id).tolist())\n",
    "    \n",
    "data_tr_vids = []\n",
    "for i in range(tweets_tr.shape[0]):\n",
    "    data_tr_vids.extend(list(vclaims_prep.keys()))\n",
    "    \n",
    "data_tr_labels = []\n",
    "for tweet_id in tweets_tr_prep.keys():\n",
    "    labels = np.zeros(vclaims.shape[0])\n",
    "    if tweet_id in qrels_tr['tweet_id'].values:\n",
    "        for index in qrels_tr[qrels_tr['tweet_id'] == tweet_id]['vclaim_id'].values:\n",
    "            labels[index] = 1\n",
    "    data_tr_labels.extend(labels)\n",
    "    \n",
    "data_tr = pd.DataFrame(columns=['tid', 'vid', 'cs_tvc', 'cs_tvt', 'bm25_tvc', 'bm25_tvt', 'label'])\n",
    "data_tr['tid'] = data_tr_tids\n",
    "data_tr['vid'] = data_tr_vids\n",
    "data_tr['cs_tvc'] = df_cs_tvc_tr.values.flatten()\n",
    "data_tr['cs_tvt'] = df_cs_tvt_tr.values.flatten()\n",
    "data_tr['bm25_tvc'] = df_bm25_tvc_tr.values.flatten()\n",
    "data_tr['bm25_tvt'] = df_bm25_tvt_tr.values.flatten()\n",
    "data_tr['label'] = data_tr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_te_tids = []\n",
    "for tweet_id in tweets_te_prep.keys():\n",
    "    data_te_tids.extend((np.ones(vclaims.shape[0]) * tweet_id).tolist())\n",
    "    \n",
    "data_te_vids = []\n",
    "for i in range(tweets_te.shape[0]):\n",
    "    data_te_vids.extend(list(vclaims_prep.keys()))\n",
    "    \n",
    "data_te_labels = []\n",
    "for tweet_id in tweets_te_prep.keys():\n",
    "    labels = np.zeros(vclaims.shape[0])\n",
    "    if tweet_id in qrels_te['tweet_id'].values:\n",
    "        for index in qrels_te[qrels_te['tweet_id'] == tweet_id]['vclaim_id'].values:\n",
    "            labels[index] = 1\n",
    "    data_te_labels.extend(labels)\n",
    "    \n",
    "data_te = pd.DataFrame(columns=['tid', 'vid', 'cs_tvc', 'cs_tvt', 'bm25_tvc', 'bm25_tvt', 'label'])\n",
    "data_te['tid'] = data_te_tids\n",
    "data_te['vid'] = data_te_vids\n",
    "data_te['cs_tvc'] = df_cs_tvc_te.values.flatten()\n",
    "data_te['cs_tvt'] = df_cs_tvt_te.values.flatten()\n",
    "data_te['bm25_tvc'] = df_bm25_tvc_te.values.flatten()\n",
    "data_te['bm25_tvt'] = df_bm25_tvt_te.values.flatten()\n",
    "data_te['label'] = data_te_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = data_tr[['cs_tvc', 'cs_tvt', 'bm25_tvc', 'bm25_tvt']].values, data_tr['label'].values\n",
    "X_te, y_te = data_te[['cs_tvc', 'cs_tvt', 'bm25_tvc', 'bm25_tvt']].values, data_te['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing class weights to handle data imbalance\n",
    "class_weight = {}\n",
    "for label in set(y_te):\n",
    "    class_weight[label] = np.sum(y_te == label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',class_weight=class_weight).fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = np.sum(X_te * clf.coef_, axis=1) + clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_score = pd.DataFrame(columns=['tid','vid', 'score'])\n",
    "result_score['tid'] = data_te['tid']\n",
    "result_score['vid'] = data_te['vid']\n",
    "result_score['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['tweet_id', 'Q0', 'vclaim_id', 'rank', 'score', 'tag'])\n",
    "\n",
    "tid = list(result_score.groupby(by='tid').groups.keys())\n",
    "for i in tid:\n",
    "    idx = result_score[result_score['tid'] == i]['score'].idxmax()\n",
    "    inf = result_score.iloc[idx]\n",
    "    result = result.append({'tweet_id':int(inf[0]), 'Q0':'Q0', 'vclaim_id':int(inf[1]),\n",
    "                            'rank':1, 'score':inf[2], 'tag': 'COVID-19'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>Q0</th>\n",
       "      <th>vclaim_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Q0</td>\n",
       "      <td>2194</td>\n",
       "      <td>1</td>\n",
       "      <td>-14.076331</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>Q0</td>\n",
       "      <td>639</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.069489</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>Q0</td>\n",
       "      <td>6368</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.520517</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>Q0</td>\n",
       "      <td>219</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.651783</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>Q0</td>\n",
       "      <td>1275</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.858168</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>960</td>\n",
       "      <td>Q0</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.676362</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>966</td>\n",
       "      <td>Q0</td>\n",
       "      <td>355</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.422657</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>968</td>\n",
       "      <td>Q0</td>\n",
       "      <td>524</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.275579</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>982</td>\n",
       "      <td>Q0</td>\n",
       "      <td>352</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.186459</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>986</td>\n",
       "      <td>Q0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.950467</td>\n",
       "      <td>COVID-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id  Q0 vclaim_id rank      score       tag\n",
       "0          0  Q0      2194    1 -14.076331  COVID-19\n",
       "1         11  Q0       639    1 -12.069489  COVID-19\n",
       "2         21  Q0      6368    1 -12.520517  COVID-19\n",
       "3         28  Q0       219    1 -12.651783  COVID-19\n",
       "4         31  Q0      1275    1 -15.858168  COVID-19\n",
       "..       ...  ..       ...  ...        ...       ...\n",
       "192      960  Q0       178    1  -7.676362  COVID-19\n",
       "193      966  Q0       355    1  -4.422657  COVID-19\n",
       "194      968  Q0       524    1  -8.275579  COVID-19\n",
       "195      982  Q0       352    1 -10.186459  COVID-19\n",
       "196      986  Q0        30    1 -13.950467  COVID-19\n",
       "\n",
       "[197 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('golf_system_result_2.csv', header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 evaluate.py -s golf_system_result_2.csv -g data/dev/tweet-vclaim-pairs.qrels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
